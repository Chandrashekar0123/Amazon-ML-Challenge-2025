# -*- coding: utf-8 -*-
"""Submission8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IAXT46e6uQ-5qxpwbQH0YJdeHkzlOhM0
"""

import re
import numpy as np
import pandas as pd
from tqdm import tqdm
import os
import time

# Suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # <<< NEW CALLBACK IMPORTS

# --- Constants ---
TRAIN_PATH = '/content/drive/MyDrive/Amazon_ML/student_resource/dataset/train.csv'
TEST_PATH = '/content/drive/MyDrive/Amazon_ML/student_resource/dataset/test.csv'
FEATURE_FOLDER = '/content/drive/MyDrive/Amazon_ML/student_resource/features' # New feature folder constant
MAX_NUM_WORDS = 30000
MAX_SEQ_LEN = 150
EMBEDDING_DIM = 128
IMAGE_FEATURE_DIM = 512 # <<< FIX: Updated dimension to 512
EPSILON = 1e-6 # Small constant for log safety and SMAPE stability
MAX_EPOCHS = 100 # <<< INCREASED MAX EPOCHS

# --- Utility Functions ---

def clean_text(text):
    """Performs standard NLP cleaning."""
    if not isinstance(text, str):
        return ""
    text = text.lower().strip()
    text = re.sub(r"<.*?>", " ", text) # Remove HTML tags
    text = re.sub(r"https?://\S+|www\.\S+", " ", text) # Remove URLs
    text = re.sub(r"[^a-z0-9\s\+\-x%./]", " ", text) # Keep useful symbols
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_ipq(text):
    """Extracts Item Pack Quantity (IPQ) using regex."""
    if not isinstance(text, str):
        return 1

    # Aggressive pattern searching for pack quantity
    patterns = [
        r'(\d+)\s*pack', r'pack of\s*(\d+)', r'(\d+)\s*ct\b',
        r'(\d+)\s*count\b', r'(\d+)\s*pcs\b', r'x\s*(\d+)\b'
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            try:
                # Return the largest captured number if multiple are found (common for specs)
                return max(1, int(match.group(1)))
            except ValueError:
                continue

    return 1 # Default assumption for single item

def smape_metric(y_true, y_pred):
    """Calculates SMAPE (Symmetric Mean Absolute Percentage Error) - Non-differentiable."""
    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_pred) + np.abs(y_true)) / 2.0
    return np.mean(numerator / np.maximum(denominator, EPSILON)) * 100

def custom_smape_loss(y_true, y_pred):
    """Keras custom loss function for SMAPE, operating on exponential values."""

    # 1. Inverse Transform the log-prices back to raw prices
    # y_true and y_pred are log-transformed, we need to exponentiate them
    y_true_raw = K.exp(y_true) - EPSILON
    y_pred_raw = K.exp(y_pred) - EPSILON

    # 2. Apply SMAPE formula
    numerator = K.abs(y_pred_raw - y_true_raw)
    denominator = (K.abs(y_true_raw) + K.abs(y_pred_raw)) / 2.0

    # K.maximum(denominator, K.epsilon()) handles potential division by zero
    return K.mean(numerator / K.maximum(denominator, K.epsilon()), axis=-1)

def download_and_get_image_features(dataset_type, df, feature_dim=IMAGE_FEATURE_DIM):
    """
    Loads pre-computed image features from the NPY files.
    """
    filename = f"X_{dataset_type}_img.npy"
    file_path = os.path.join(FEATURE_FOLDER, filename)

    if os.path.exists(file_path):
        print(f"--- Loading REAL Image Features from: {file_path} ---")
        try:
            # We assume the loaded data matches the IMAGE_FEATURE_DIM (now 512)
            loaded_features = np.load(file_path)
            if loaded_features.shape[1] != feature_dim:
                print(f"WARNING: Loaded features shape {loaded_features.shape[1]} does not match expected {feature_dim}. Using loaded data anyway.")
            return loaded_features
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            print("Falling back to MOCKING features.")
            np.random.seed(42)
            # Use the correct, updated feature_dim for the mock data
            return np.random.rand(df.shape[0], feature_dim).astype(np.float32)
    else:
        print(f"--- File not found: {file_path}. Generating MOCK Image Features ---")
        print("NOTE: This step is CRUCIAL for low SMAPE. Replace with real data.")
        np.random.seed(42)
        # Use the correct, updated feature_dim for the mock data
        return np.random.rand(df.shape[0], feature_dim).astype(np.float32)


def build_multi_modal_model(input_dim_ipq, input_dim_img, max_words, max_seq_len, embedding_dim):
    """Builds the Keras Functional API model combining Text, IPQ, and Image features."""

    # 1. Text Branch (Convolutional Neural Network for Text)
    text_input = Input(shape=(max_seq_len,), name='text_input')
    text_emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_len)(text_input)
    # Using Conv1D and GlobalMaxPooling is often faster and effective than LSTM for text features
    text_conv = Conv1D(filters=128, kernel_size=5, activation='relu')(text_emb)
    text_pool = GlobalMaxPooling1D()(text_conv)
    text_drop = Dropout(0.3)(text_pool)
    text_dense = Dense(64, activation='relu')(text_drop)

    # 2. IPQ (Structural) Branch
    ipq_input = Input(shape=(input_dim_ipq,), name='ipq_input')
    ipq_dense = Dense(32, activation='relu')(ipq_input)
    ipq_drop = Dropout(0.1)(ipq_dense)

    # 3. Image (Visual) Branch
    # The input shape here now correctly expects 512 features
    img_input = Input(shape=(input_dim_img,), name='img_input')
    img_dense = Dense(128, activation='relu')(img_input)
    img_drop = Dropout(0.3)(img_dense)

    # 4. Combine Branches
    combined = Concatenate()([text_dense, ipq_drop, img_drop])

    # 5. Final Regression Head
    dense_final = Dense(64, activation='relu')(combined)
    dense_final = Dropout(0.2)(dense_final)
    output = Dense(1, activation='linear')(dense_final) # Predicts log_price

    model = Model(inputs=[text_input, ipq_input, img_input], outputs=output)

    # Compile using the custom SMAPE loss and a focused optimizer
    model.compile(optimizer='adam', loss=custom_smape_loss)

    return model

# --- Main Execution ---

def main():

    # Load Data
    try:
        train = pd.read_csv(TRAIN_PATH)
        test = pd.read_csv(TEST_PATH)
    except Exception as e:
        print(f"Error loading data. Ensure paths are correct and files exist. Error: {e}")
        return

    # --- 1. Feature Engineering ---

    # Target Transformation: Log(price) for better model convergence
    train['log_price'] = np.log(train['price'].values + EPSILON)
    y_train_log = train['log_price'].values

    # Text Cleaning and IPQ Extraction
    print("1. Cleaning Text and Extracting IPQ...")
    for df in [train, test]:
        df['catalog_content'] = df['catalog_content'].fillna("")
        df['clean_text'] = df['catalog_content'].apply(clean_text)
        df['ipq'] = df['catalog_content'].apply(extract_ipq)

    # IPQ Scaling (Structural Feature)
    ipq_scaler = StandardScaler()
    X_train_ipq = ipq_scaler.fit_transform(train[['ipq']])
    X_test_ipq = ipq_scaler.transform(test[['ipq']])

    # Image Feature Loading
    X_train_img = download_and_get_image_features('train', train)
    X_test_img = download_and_get_image_features('test', test)

    # --- 2. NLP Processing ---

    print("3. Tokenizing and Padding Text...")
    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(train['clean_text'])

    X_train_seq = tokenizer.texts_to_sequences(train['clean_text'])
    X_test_seq = tokenizer.texts_to_sequences(test['clean_text'])

    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

    # --- 3. Train/Validation Split ---

    X_tr_text, X_val_text, y_tr, y_val = train_test_split(X_train_pad, y_train_log, test_size=0.1, random_state=42)
    X_tr_ipq, X_val_ipq, _, _ = train_test_split(X_train_ipq, y_train_log, test_size=0.1, random_state=42)
    X_tr_img, X_val_img, _, _ = train_test_split(X_train_img, y_train_log, test_size=0.1, random_state=42)

    # --- 4. Model Building and Training ---

    print("4. Building and Training Multi-Modal Model with Custom SMAPE Loss...")
    model = build_multi_modal_model(X_train_ipq.shape[1], IMAGE_FEATURE_DIM, MAX_NUM_WORDS, MAX_SEQ_LEN, EMBEDDING_DIM)

    # Define Callbacks for robust optimization
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5, # Stop if validation loss doesn't improve for 5 epochs
        restore_best_weights=True, # Revert to the best epoch's weights
        verbose=1
    )
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5, # Reduce learning rate by 50%
        patience=3, # If loss plateaus for 3 epochs
        min_lr=1e-6,
        verbose=1
    )
    callbacks_list = [early_stopping, reduce_lr]

    history = model.fit(
        # The input data shapes (None, 512) now correctly match the model's expected input shape
        {'text_input': X_tr_text, 'ipq_input': X_tr_ipq, 'img_input': X_tr_img}, y_tr,
        validation_data=({'text_input': X_val_text, 'ipq_input': X_val_ipq, 'img_input': X_val_img}, y_val),
        epochs=MAX_EPOCHS, # Use high number of epochs, letting EarlyStopping manage duration
        batch_size=128,
        callbacks=callbacks_list, # <<< CALL BACKS ADDED HERE
        verbose=1
    )

    # --- 5. Validation Check ---

    # Predict on validation set (log price)
    y_val_pred_log = model.predict({'text_input': X_val_text, 'ipq_input': X_val_ipq, 'img_input': X_val_img}).flatten()

    # Inverse transform to raw price
    y_val_true_raw = np.exp(y_val) - EPSILON
    y_val_pred_raw = np.exp(y_val_pred_log) - EPSILON
    y_val_pred_raw = np.maximum(y_val_pred_raw, 0.01) # Ensure positive

    # Calculate SMAPE
    val_smape = smape_metric(y_val_true_raw, y_val_pred_raw)
    print(f"\nâœ… Validation SMAPE (on {len(y_val)} samples): {val_smape:.4f}%")

    # --- 5.5. Full Training Data Check (All Data) ---
    print("\nðŸ“Š Calculating SMAPE on FULL Training Dataset (75k samples)...")

    # Predict on entire training set (log price)
    y_full_train_pred_log = model.predict({
        'text_input': X_train_pad,
        'ipq_input': X_train_ipq,
        'img_input': X_train_img
    }).flatten()

    # Inverse transform to raw price
    y_full_train_true_raw = np.exp(y_train_log) - EPSILON
    y_full_train_pred_raw = np.exp(y_full_train_pred_log) - EPSILON
    y_full_train_pred_raw = np.maximum(y_full_train_pred_raw, 0.01)

    # Calculate SMAPE
    full_train_smape = smape_metric(y_full_train_true_raw, y_full_train_pred_raw)
    print(f"âœ… FULL Training SMAPE (All Data): {full_train_smape:.4f}%")

    # --- 6. Final Prediction and Submission ---

    print("6. Generating Final Test Predictions...")
    # Predict on the actual test set (log price)
    test_preds_log = model.predict({'text_input': X_test_pad, 'ipq_input': X_test_ipq, 'img_input': X_test_img}).flatten()

    # Inverse Transform
    test_preds = np.exp(test_preds_log) - EPSILON

    # Constraint: Predicted prices must be positive float values.
    test_preds = np.maximum(test_preds, 0.01)

    submission = pd.DataFrame({
        "sample_id": test['sample_id'],
        "price": test_preds
    })

    submission.to_csv("test_out.csv", index=False)
    print("\nðŸ“¦ Submission file 'test_out.csv' created successfully!")

if __name__ == '__main__':
    # Increase precision for printing
    np.set_printoptions(precision=4)

    main()

