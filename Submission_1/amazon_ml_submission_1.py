# -*- coding: utf-8 -*-
"""Amazon-ML-Submission-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BWzI_zkTVlVV22J8pUb_0neEupn3Klpn
"""

from google.colab import drive
drive.mount('/content/drive')

import re
import string
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

import pandas as pd

# Show full content in each cell
pd.set_option('display.max_colwidth', None)

train = pd.read_csv('/content/drive/MyDrive/Amazon_ML/student_resource/dataset/train.csv')
train.head()



test = pd.read_csv('/content/drive/MyDrive/Amazon_ML/student_resource/dataset/test.csv')
test.head()

print("Train shape:", train.shape)
print("Test shape:", test.shape)
train.head()

train.isnull().sum()

# Function to show details
def dataset_info(name, df):
    print(f"üìä Dataset: {name}")
    print("="*50)
    print("Shape (rows, columns):", df.shape)
    print("\nColumn Names:", df.columns.tolist())
    print("\nData Types:\n", df.dtypes)
    print("\nMissing Values:\n", df.isnull().sum())
    print("\nFirst 5 Rows:\n", df.head())
    print("\n" + "="*50 + "\n")

# Show details for both
dataset_info("Trainung Data Information", train)

dataset_info("Testing Data Information", test)



# ===============================
# 1Ô∏è‚É£ Text Cleaning Function
# ===============================
def clean_text(text):

    if not isinstance(text, str):
        return ""

    # Lowercase and trim
    text = text.lower().strip()

    # Remove HTML tags
    text = re.sub(r"<.*?>", " ", text)

    # Remove URLs
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)

    # Remove emojis and non-standard unicode symbols
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002700-\U000027BF"  # dingbats
        u"\U0001F900-\U0001F9FF"  # supplemental symbols
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # Keep letters, numbers, and meaningful symbols: + - x % . /
    text = re.sub(r"[^a-z0-9\s\+\-x%./]", " ", text)

    # Collapse multiple spaces/newlines into single space
    text = re.sub(r"\s+", " ", text).strip()

    return text

# ===============================
# 2Ô∏è‚É£ Apply Preprocessing
# ===============================
train['clean_text'] = train['catalog_content'].fillna("").apply(clean_text)
test['clean_text'] = test['catalog_content'].fillna("").apply(clean_text)



# ===============================
# 3Ô∏è‚É£ Tokenization & Padding
# ===============================
MAX_NUM_WORDS = 20000
MAX_SEQ_LEN = 100  # max words per product description

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(train['clean_text'])

X_train_seq = tokenizer.texts_to_sequences(train['clean_text'])
X_test_seq = tokenizer.texts_to_sequences(test['clean_text'])

X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

# ===============================
# 4Ô∏è‚É£ Train/Validation Split
# ===============================
y_train = train['price'].values
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train_pad, y_train, test_size=0.2, random_state=42
)

# ===============================
# 5Ô∏è‚É£ Build LSTM Model
# ===============================
EMBEDDING_DIM = 100

model = Sequential([
    Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQ_LEN),
    LSTM(128, return_sequences=False),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(1, activation='linear')  # regression output
])

model.compile(optimizer='adam', loss='mean_absolute_error')
model.summary()

# ===============================
# 6Ô∏è‚É£ Train the Model
# ===============================
history = model.fit(
    X_tr, y_tr,
    validation_data=(X_val, y_val),
    epochs=5,
    batch_size=128
)

# ===============================
# 7Ô∏è‚É£ Validate
# ===============================
y_val_pred = model.predict(X_val).flatten()
mae = mean_absolute_error(y_val, y_val_pred)
print(f"Validation MAE: {mae:.2f}")

# ===============================
# 8Ô∏è‚É£ Predict on Test Set
# ===============================
test_preds = model.predict(X_test_pad).flatten()
test_preds = np.maximum(test_preds, 0)  # Ensure positive prices

# Performance Metrics

def smape(y_true, y_pred):
    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_pred) + np.abs(y_true)) / 2
    smape_val = np.mean(numerator / denominator) * 100
    return smape_val

# Example
y_true = np.array([100, 200, 300])
y_pred = np.array([120, 190, 310])

print("SMAPE:", smape(y_true, y_pred))

# ===============================
# 10 Prepare Submission
# ===============================
submission = pd.DataFrame({
    "sample_id": test['sample_id'],
    "price": test_preds
})

submission.to_csv("test_out.csv", index=False)
print("Submission file 'test_out.csv' created successfully!")





