{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHqmH-zqDHcC",
        "outputId": "3fef68bd-44d5-4f5d-e073-fd7892d3b66b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Constants ---\n",
        "TRAIN_PATH = '/content/drive/MyDrive/Amazon_ML/student_resource/dataset/train.csv'\n",
        "TEST_PATH = '/content/drive/MyDrive/Amazon_ML/student_resource/dataset/test.csv'\n",
        "MAX_NUM_WORDS = 30000\n",
        "MAX_SEQ_LEN = 150\n",
        "EMBEDDING_DIM = 128\n",
        "IMAGE_FEATURE_DIM = 2048 # Dimension expected from a pre-trained ResNet50/EfficientNet\n",
        "EPSILON = 1e-6 # Small constant for log safety and SMAPE stability\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Performs standard NLP cleaning.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"<.*?>\", \" \", text) # Remove HTML tags\n",
        "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text) # Remove URLs\n",
        "    text = re.sub(r\"[^a-z0-9\\s\\+\\-x%./]\", \" \", text) # Keep useful symbols\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def extract_ipq(text):\n",
        "    \"\"\"Extracts Item Pack Quantity (IPQ) using regex.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return 1\n",
        "\n",
        "    # Aggressive pattern searching for pack quantity\n",
        "    patterns = [\n",
        "        r'(\\d+)\\s*pack', r'pack of\\s*(\\d+)', r'(\\d+)\\s*ct\\b',\n",
        "        r'(\\d+)\\s*count\\b', r'(\\d+)\\s*pcs\\b', r'x\\s*(\\d+)\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                # Return the largest captured number if multiple are found (common for specs)\n",
        "                return max(1, int(match.group(1)))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return 1 # Default assumption for single item\n",
        "\n",
        "def smape_metric(y_true, y_pred):\n",
        "    \"\"\"Calculates SMAPE (Symmetric Mean Absolute Percentage Error) - Non-differentiable.\"\"\"\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_pred) + np.abs(y_true)) / 2.0\n",
        "    return np.mean(numerator / np.maximum(denominator, EPSILON)) * 100\n",
        "\n",
        "def custom_smape_loss(y_true, y_pred):\n",
        "    \"\"\"Keras custom loss function for SMAPE, operating on exponential values.\"\"\"\n",
        "\n",
        "    # 1. Inverse Transform the log-prices back to raw prices\n",
        "    # y_true and y_pred are log-transformed, we need to exponentiate them\n",
        "    y_true_raw = K.exp(y_true) - EPSILON\n",
        "    y_pred_raw = K.exp(y_pred) - EPSILON\n",
        "\n",
        "    # 2. Apply SMAPE formula\n",
        "    numerator = K.abs(y_pred_raw - y_true_raw)\n",
        "    denominator = (K.abs(y_true_raw) + K.abs(y_pred_raw)) / 2.0\n",
        "\n",
        "    # K.maximum(denominator, K.epsilon()) handles potential division by zero\n",
        "    return K.mean(numerator / K.maximum(denominator, K.epsilon()), axis=-1)\n",
        "\n",
        "def download_and_get_image_features(df, feature_dim=IMAGE_FEATURE_DIM):\n",
        "    \"\"\"\n",
        "    CONCEPTUAL FUNCTION: In a real submission, this function must:\n",
        "    1. Use src/utils.py to download images from image_link.\n",
        "    2. Load a pre-trained CNN (e.g., ResNet50) without its top layers.\n",
        "    3. Process each image and extract the feature vector from the penultimate layer.\n",
        "\n",
        "    Here, we MOCK the process with random data for pipeline completeness.\n",
        "    \"\"\"\n",
        "    print(\"--- MOCKING IMAGE FEATURE EXTRACTION (Crucial Step) ---\")\n",
        "    # For robust results, replace this with actual CNN feature extraction.\n",
        "    np.random.seed(42)\n",
        "    return np.random.rand(df.shape[0], feature_dim).astype(np.float32)\n",
        "\n",
        "def build_multi_modal_model(input_dim_ipq, input_dim_img, max_words, max_seq_len, embedding_dim):\n",
        "    \"\"\"Builds the Keras Functional API model combining Text, IPQ, and Image features.\"\"\"\n",
        "\n",
        "    # 1. Text Branch (Convolutional Neural Network for Text)\n",
        "    text_input = Input(shape=(max_seq_len,), name='text_input')\n",
        "    text_emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_len)(text_input)\n",
        "    # Using Conv1D and GlobalMaxPooling is often faster and effective than LSTM for text features\n",
        "    text_conv = Conv1D(filters=128, kernel_size=5, activation='relu')(text_emb)\n",
        "    text_pool = GlobalMaxPooling1D()(text_conv)\n",
        "    text_drop = Dropout(0.3)(text_pool)\n",
        "    text_dense = Dense(64, activation='relu')(text_drop)\n",
        "\n",
        "    # 2. IPQ (Structural) Branch\n",
        "    ipq_input = Input(shape=(input_dim_ipq,), name='ipq_input')\n",
        "    ipq_dense = Dense(32, activation='relu')(ipq_input)\n",
        "    ipq_drop = Dropout(0.1)(ipq_dense)\n",
        "\n",
        "    # 3. Image (Visual) Branch\n",
        "    img_input = Input(shape=(input_dim_img,), name='img_input')\n",
        "    img_dense = Dense(128, activation='relu')(img_input)\n",
        "    img_drop = Dropout(0.3)(img_dense)\n",
        "\n",
        "    # 4. Combine Branches\n",
        "    combined = Concatenate()([text_dense, ipq_drop, img_drop])\n",
        "\n",
        "    # 5. Final Regression Head\n",
        "    dense_final = Dense(64, activation='relu')(combined)\n",
        "    dense_final = Dropout(0.2)(dense_final)\n",
        "    output = Dense(1, activation='linear')(dense_final) # Predicts log_price\n",
        "\n",
        "    model = Model(inputs=[text_input, ipq_input, img_input], outputs=output)\n",
        "\n",
        "    # Compile using the custom SMAPE loss and a focused optimizer\n",
        "    model.compile(optimizer='adam', loss=custom_smape_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Load Data\n",
        "    try:\n",
        "        train = pd.read_csv(TRAIN_PATH)\n",
        "        test = pd.read_csv(TEST_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data. Ensure paths are correct and files exist. Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 1. Feature Engineering ---\n",
        "\n",
        "    # Target Transformation: Log(price) for better model convergence\n",
        "    train['log_price'] = np.log(train['price'].values + EPSILON)\n",
        "    y_train_log = train['log_price'].values\n",
        "\n",
        "    # Text Cleaning and IPQ Extraction\n",
        "    print(\"1. Cleaning Text and Extracting IPQ...\")\n",
        "    for df in [train, test]:\n",
        "        df['catalog_content'] = df['catalog_content'].fillna(\"\")\n",
        "        df['clean_text'] = df['catalog_content'].apply(clean_text)\n",
        "        df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
        "\n",
        "    # IPQ Scaling (Structural Feature)\n",
        "    ipq_scaler = StandardScaler()\n",
        "    X_train_ipq = ipq_scaler.fit_transform(train[['ipq']])\n",
        "    X_test_ipq = ipq_scaler.transform(test[['ipq']])\n",
        "\n",
        "    # Image Feature Extraction (MOCK)\n",
        "    print(\"2. Generating MOCK Image Features (Must be replaced with actual CNN extraction)...\")\n",
        "    X_train_img = download_and_get_image_features(train)\n",
        "    X_test_img = download_and_get_image_features(test)\n",
        "\n",
        "    # --- 2. NLP Processing ---\n",
        "\n",
        "    print(\"3. Tokenizing and Padding Text...\")\n",
        "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(train['clean_text'])\n",
        "\n",
        "    X_train_seq = tokenizer.texts_to_sequences(train['clean_text'])\n",
        "    X_test_seq = tokenizer.texts_to_sequences(test['clean_text'])\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "\n",
        "    # --- 3. Train/Validation Split ---\n",
        "\n",
        "    X_tr_text, X_val_text, y_tr, y_val = train_test_split(X_train_pad, y_train_log, test_size=0.1, random_state=42)\n",
        "    X_tr_ipq, X_val_ipq, _, _ = train_test_split(X_train_ipq, y_train_log, test_size=0.1, random_state=42)\n",
        "    X_tr_img, X_val_img, _, _ = train_test_split(X_train_img, y_train_log, test_size=0.1, random_state=42)\n",
        "\n",
        "    # --- 4. Model Building and Training ---\n",
        "\n",
        "    print(\"4. Building and Training Multi-Modal Model with Custom SMAPE Loss...\")\n",
        "    model = build_multi_modal_model(X_train_ipq.shape[1], IMAGE_FEATURE_DIM, MAX_NUM_WORDS, MAX_SEQ_LEN, EMBEDDING_DIM)\n",
        "\n",
        "    history = model.fit(\n",
        "        {'text_input': X_tr_text, 'ipq_input': X_tr_ipq, 'img_input': X_tr_img}, y_tr,\n",
        "        validation_data=({'text_input': X_val_text, 'ipq_input': X_val_ipq, 'img_input': X_val_img}, y_val),\n",
        "        epochs=15, # Increased epochs for better convergence\n",
        "        batch_size=128,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- 5. Validation Check ---\n",
        "\n",
        "    # Predict on validation set (log price)\n",
        "    y_val_pred_log = model.predict({'text_input': X_val_text, 'ipq_input': X_val_ipq, 'img_input': X_val_img}).flatten()\n",
        "\n",
        "    # Inverse transform to raw price\n",
        "    y_val_true_raw = np.exp(y_val) - EPSILON\n",
        "    y_val_pred_raw = np.exp(y_val_pred_log) - EPSILON\n",
        "    y_val_pred_raw = np.maximum(y_val_pred_raw, 0.01) # Ensure positive\n",
        "\n",
        "    # Calculate SMAPE\n",
        "    val_smape = smape_metric(y_val_true_raw, y_val_pred_raw)\n",
        "    print(f\"\\nâœ… Validation SMAPE (on {len(y_val)} samples): {val_smape:.4f}%\")\n",
        "\n",
        "    # --- 6. Final Prediction and Submission ---\n",
        "\n",
        "    print(\"5. Generating Final Test Predictions...\")\n",
        "    # Predict on the actual test set (log price)\n",
        "    test_preds_log = model.predict({'text_input': X_test_pad, 'ipq_input': X_test_ipq, 'img_input': X_test_img}).flatten()\n",
        "\n",
        "    # Inverse Transform\n",
        "    test_preds = np.exp(test_preds_log) - EPSILON\n",
        "\n",
        "    # Constraint: Predicted prices must be positive float values.\n",
        "    test_preds = np.maximum(test_preds, 0.01)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        \"sample_id\": test['sample_id'],\n",
        "        \"price\": test_preds\n",
        "    })\n",
        "\n",
        "    submission.to_csv(\"test_out.csv\", index=False)\n",
        "    print(\"\\nğŸ“¦ Submission file 'test_out.csv' created successfully!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Increase precision for printing\n",
        "    np.set_printoptions(precision=4)\n",
        "\n",
        "    # Ensure all TensorFlow operations are run\n",
        "    # K.clear_session()\n",
        "\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Cleaning Text and Extracting IPQ...\n",
            "2. Generating MOCK Image Features (Must be replaced with actual CNN extraction)...\n",
            "--- MOCKING IMAGE FEATURE EXTRACTION (Crucial Step) ---\n",
            "--- MOCKING IMAGE FEATURE EXTRACTION (Crucial Step) ---\n",
            "3. Tokenizing and Padding Text...\n",
            "4. Building and Training Multi-Modal Model with Custom SMAPE Loss...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 262ms/step - loss: 0.7385 - val_loss: 0.5680\n",
            "Epoch 2/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 249ms/step - loss: 0.5498 - val_loss: 0.5392\n",
            "Epoch 3/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 254ms/step - loss: 0.5050 - val_loss: 0.5110\n",
            "Epoch 4/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 252ms/step - loss: 0.4726 - val_loss: 0.5141\n",
            "Epoch 5/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 253ms/step - loss: 0.4449 - val_loss: 0.4983\n",
            "Epoch 6/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 253ms/step - loss: 0.4268 - val_loss: 0.5005\n",
            "Epoch 7/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 254ms/step - loss: 0.4102 - val_loss: 0.5090\n",
            "Epoch 8/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 249ms/step - loss: 0.3909 - val_loss: 0.5030\n",
            "Epoch 9/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 251ms/step - loss: 0.3753 - val_loss: 0.5101\n",
            "Epoch 10/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 254ms/step - loss: 0.3646 - val_loss: 0.5020\n",
            "Epoch 11/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 252ms/step - loss: 0.3510 - val_loss: 0.4999\n",
            "Epoch 12/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 250ms/step - loss: 0.3414 - val_loss: 0.5153\n",
            "Epoch 13/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 253ms/step - loss: 0.3290 - val_loss: 0.5027\n",
            "Epoch 14/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 253ms/step - loss: 0.3238 - val_loss: 0.4962\n",
            "Epoch 15/15\n",
            "\u001b[1m528/528\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 247ms/step - loss: 0.3122 - val_loss: 0.4995\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\n",
            "\n",
            "âœ… Validation SMAPE (on 7500 samples): 49.9480%\n",
            "5. Generating Final Test Predictions...\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step\n",
            "\n",
            "ğŸ“¦ Submission file 'test_out.csv' created successfully!\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZvGtjWSC9qm",
        "outputId": "23a90166-eabd-436a-b3ff-91271d8dfebc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Constant required for the SMAPE function ---\n",
        "EPSILON = 1e-6\n",
        "\n",
        "def smape_metric(y_true, y_pred):\n",
        "    \"\"\"Calculates SMAPE (Symmetric Mean Absolute Percentage Error).\"\"\"\n",
        "\n",
        "    # Ensure y_pred is positive, as is done in your submission logic\n",
        "    y_pred = np.maximum(y_pred, EPSILON)\n",
        "\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_pred) + np.abs(y_true)) / 2.0\n",
        "\n",
        "    # Use np.maximum to prevent division by zero/near zero\n",
        "    return np.mean(numerator / np.maximum(denominator, EPSILON)) * 100\n",
        "\n",
        "# --- Example Usage (Mirroring the Validation Check in your code) ---\n",
        "\n",
        "# Example Actual Raw Prices (y_val_true_raw in your script)\n",
        "y_actual_raw = np.array([100.0, 250.0, 99.5, 400.0])\n",
        "\n",
        "# Example Predicted Raw Prices (y_val_pred_raw in your script)\n",
        "# A very low SMAPE means predictions are very close to actuals\n",
        "y_predicted_raw = np.array([102.5, 248.0, 99.0, 405.0])\n",
        "\n",
        "calculated_smape = smape_metric(y_actual_raw, y_predicted_raw)\n",
        "\n",
        "print(f\"Example Actual Prices: {y_actual_raw}\")\n",
        "print(f\"Example Predicted Prices: {y_predicted_raw}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Calculated SMAPE Score: {calculated_smape:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u83ADIusLzHH",
        "outputId": "9dc0ae73-fecd-46b3-bb8d-cf4088a8f340"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Actual Prices: [100.  250.   99.5 400. ]\n",
            "Example Predicted Prices: [102.5 248.   99.  405. ]\n",
            "------------------------------\n",
            "Calculated SMAPE Score: 1.2546%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}