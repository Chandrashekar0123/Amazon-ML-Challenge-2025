# -*- coding: utf-8 -*-
"""Multimodal Price Prediction Ensemble

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lGdHDleBoQ-KQO984rHf74vTiMlY9p_-
"""

import pandas as pd
import numpy as np
import re
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
# NOTE: In a full environment, you would also import PyTorch/TensorFlow and deep learning utilities

# --- HELPER UTILITY (PLACEHOLDER FOR COMPETITION FILE) ---
# NOTE: The competition requires using a file named src/utils.py for image download.
# You MUST replace this placeholder with the actual implementation provided by the organizers.
def download_images(image_links, output_dir='images'):
    """
    PLACEHOLDER: Simulates the image download function from src/utils.py.
    In the real solution, this function handles downloading the images
    from the provided URLs in 'image_link'.
    """
    import os
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print(f"--- NOTE: Running placeholder for image download. Image features will be mocked. ---")
    print(f"--- You must implement or import the actual 'download_images' from src/utils.py ---")
    # This function would typically return a list of local file paths for the images.
    return [os.path.join(output_dir, f"{i}.jpg") for i in range(len(image_links))]


# --- 1. EVALUATION METRIC ---

def calculate_smape(y_true, y_pred):
    """
    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).
    Formula: (1/n) * Σ(|y_true - y_pred| / ((|y_true| + |y_pred|)/2)) * 100
    """
    # Ensure all predictions are positive (as price must be positive)
    y_pred[y_pred < 0] = 0.0

    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2

    # Handle the case where both true and pred are zero (SMAPE should be 0)
    # This is less likely with real prices, but good practice.
    mask = (y_true == 0) & (y_pred == 0)
    denominator[mask] = 1 # Avoid division by zero, numerator will be 0 anyway.

    smape = np.mean(numerator / denominator) * 100
    return smape

# --- 2. DATA UTILITIES (ACTUAL DATA LOADING) ---

def load_datasets():
    """
    Loads the actual training and testing datasets from the expected file paths.

    The file paths are defined based on the challenge description:
    - Training file: dataset/train.csv
    - Test file: dataset/test.csv
    """
    TRAIN_PATH = '/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/dataset/train.csv'
    TEST_PATH = '/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/dataset/test.csv'

    print("Loading actual datasets from CSV files...")

    try:
        # Load Training Data (with 'price' label)
        train_df = pd.read_csv(TRAIN_PATH)
        print(f"Loaded training data: {len(train_df)} samples from {TRAIN_PATH}")

        # Load Test Data (without 'price' label)
        test_df = pd.read_csv(TEST_PATH)
        print(f"Loaded test data: {len(test_df)} samples from {TEST_PATH}")

        # Apply the required log transformation for the target variable (price)
        if 'price' in train_df.columns:
            # We use log1p (log(1+x)) to handle potential zero prices robustly
            train_df['log_price'] = np.log1p(train_df['price'])
        else:
            print("WARNING: 'price' column not found in training data. Cannot create log_price target.")

        return train_df, test_df

    except FileNotFoundError as e:
        print(f"ERROR: Could not find dataset file. Please ensure {TRAIN_PATH} and {TEST_PATH} exist in your project structure.")
        raise e


# --- 3. FEATURE ENGINEERING MODULE ---

def extract_ipq(text):
    """
    Extracts Item Pack Quantity (IPQ) from the catalog content using regex.
    This is a simplified example; real-world IPQ extraction would be more robust.
    """
    # Look for patterns like 'Pack of 50 bags', '1 monitor', '5 lbs', '30 servings'
    ipq_match = re.search(r'IPQ:\s*(?:Pack of )?(\d+\s*[\w\.]+)', text, re.IGNORECASE)
    if ipq_match:
        return ipq_match.group(1).strip()
    return np.nan

def engineer_features(df, vectorizer=None):
    """
    Engineers tabular and text features.

    Args:
        df (pd.DataFrame): Input dataframe (train or test).
        vectorizer (TfidfVectorizer): Pre-fitted TFIDF vectorizer (used for test data).

    Returns:
        tuple: (df with engineered features, TFIDF features, fitted vectorizer)
    """
    print("Starting feature engineering...")

    # Ensure all content is string and handle NaNs before processing
    df['catalog_content'] = df['catalog_content'].fillna('').astype(str)

    # 1. IPQ Extraction (Numerical Feature)
    df['raw_ipq'] = df['catalog_content'].apply(extract_ipq)

    # Extract numerical part of IPQ (very simplified for this example)
    df['ipq_num'] = df['raw_ipq'].str.extract(r'(\d+)').astype(float)
    df['ipq_num'] = df['ipq_num'].fillna(1.0)

    # 2. Brand Extraction (Categorical Feature)
    # Mock brand extraction based on the first word after 'Title:'
    df['brand'] = df['catalog_content'].apply(
        lambda x: x.split('Title:')[1].strip().split(' ')[0].replace(',', '').replace('.', '')
        if isinstance(x, str) and 'Title:' in x else 'Unknown'
    )

    # Simple Label Encoding for the categorical Brand feature
    df['brand_encoded'] = df['brand'].astype('category').cat.codes

    # 3. TFIDF Vectorization (Textual Feature)
    if vectorizer is None:
        # Fit vectorizer on training data content
        vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))
        tfidf_features = vectorizer.fit_transform(df['catalog_content'])
    else:
        # Transform test data using the fitted vectorizer
        tfidf_features = vectorizer.transform(df['catalog_content'])

    return df, tfidf_features, vectorizer

def get_image_features(df, feature_size=2048):
    """
    MOCK FUNCTION: Simulates feature extraction from images using a pre-trained CNN.

    In a real solution:
    1. Call `download_images(df['image_link'])` to get local file paths.
    2. Load a pre-trained CNN (e.g., ResNet50, EfficientNet, constrained by 8B parameters).
    3. Process each image through the CNN to get the final layer's feature vector.

    Returns:
        np.array: A matrix of deep visual features.
    """
    print(f"--- MOCKING IMAGE FEATURE EXTRACTION ---")

    # In a real environment, you would call:
    # image_paths = download_images(df['image_link'])

    print(f"Using placeholder features of size {feature_size}. In production, this must be a CNN output.")

    # Placeholder: Random features to simulate VGG/ResNet output
    np.random.seed(42)
    features = np.random.rand(len(df), feature_size)
    return features

# --- 4. MULTIMODAL ENSEMBLE MODELING ---

def train_and_predict_ensemble(train_df, test_df, X_train_tfidf, X_test_tfidf, X_train_img, X_test_img):
    """
    Trains the three components (Text GBM, Image NN Placeholder, Ensemble Ridge)
    and generates final predictions.
    """

    # Define features for the GBM (Text Model)
    GBM_FEATURES = ['ipq_num', 'brand_encoded', 'tfidf_sum', 'tfidf_mean']

    # ----------------------------------------------------
    # PHASE 1: TEXT MODEL (LightGBM)
    # ----------------------------------------------------
    print("\n--- Training Text Model (LightGBM) ---")

    # Augment tabular features with simple TFIDF aggregations
    train_df['tfidf_sum'] = X_train_tfidf.sum(axis=1)
    train_df['tfidf_mean'] = X_train_tfidf.mean(axis=1)

    lgb_params = {
        'objective': 'regression_l1', # MAE is a good proxy for SMAPE optimization
        'metric': 'mae',
        'n_estimators': 1000,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 1,
        'verbose': -1,
        'n_jobs': -1,
        'seed': 42
    }

    X_train_gbm = train_df[GBM_FEATURES].values
    y_train = train_df['log_price'].values

    text_model = lgb.LGBMRegressor(**lgb_params)
    text_model.fit(X_train_gbm, y_train)

    # Predict OOF (Out-Of-Fold) for stacking
    text_oof = np.zeros(len(train_df))
    # Note: KFold should be used for robust OOF predictions in production.
    # For simplicity, we use a single prediction here.
    text_oof = text_model.predict(X_train_gbm)

    # Apply TFIDF aggregations to test data for prediction
    test_df['tfidf_sum'] = X_test_tfidf.sum(axis=1)
    test_df['tfidf_mean'] = X_test_tfidf.mean(axis=1)

    text_test_pred = text_model.predict(test_df[GBM_FEATURES].values)

    # ----------------------------------------------------
    # PHASE 2: IMAGE MODEL (Mock Simple FFNN/Linear Model on Image Features)
    # ----------------------------------------------------
    print("\n--- Training Image Model (Linear Regression on CNN Features) ---")

    # Standardize image features for the linear model
    img_scaler = StandardScaler()
    X_train_img_scaled = img_scaler.fit_transform(X_train_img)
    X_test_img_scaled = img_scaler.transform(X_test_img)

    # Use Ridge Regression as a simple, stable model for image features
    image_model = Ridge(alpha=1.0, random_state=42)
    image_model.fit(X_train_img_scaled, y_train)

    image_oof = image_model.predict(X_train_img_scaled)
    image_test_pred = image_model.predict(X_test_img_scaled)

    # ----------------------------------------------------
    # PHASE 3: STACKING / ENSEMBLE MODEL (Meta-Model)
    # ----------------------------------------------------
    print("\n--- Training Ensemble Meta-Model (Stacking) ---")

    # Create the Meta-Features (predictions from base models + key original features)
    X_meta_train = pd.DataFrame({
        'text_pred': text_oof,
        'image_pred': image_oof,
        'ipq_num': train_df['ipq_num'].values
    })

    X_meta_test = pd.DataFrame({
        'text_pred': text_test_pred,
        'image_pred': image_test_pred,
        'ipq_num': test_df['ipq_num'].fillna(train_df['ipq_num'].mean()).values # Handle missing IPQ in test
    })

    # The final ensemble model (Ridge Regression for stable, weighted combination)
    ensemble_model = Ridge(alpha=0.5, random_state=42)
    ensemble_model.fit(X_meta_train, y_train)

    # Final Prediction on the log-scale
    final_log_preds = ensemble_model.predict(X_meta_test)

    # ----------------------------------------------------
    # PHASE 4: INVERSE TRANSFORM AND OUTPUT
    # ----------------------------------------------------

    # Inverse transform (exp(log_price) - 1) to get the final price
    final_price_preds = np.expm1(final_log_preds)

    # Ensure prices are positive floats as per constraint
    final_price_preds[final_price_preds < 0] = 0.01

    return final_price_preds


# --- 5. MAIN EXECUTION ---

def run_solution():
    """Main function to run the entire pricing solution."""

    # 1. Load Data
    train_df, test_df = load_datasets()

    # 2. Feature Engineering (Text/Tabular)
    train_df, X_train_tfidf, tfidf_vectorizer = engineer_features(train_df)
    test_df, X_test_tfidf, _ = engineer_features(test_df, vectorizer=tfidf_vectorizer)

    # 3. Image Feature Extraction (MOCK/Placeholder)
    # NOTE: This is the most computationally expensive and critical part in a real solution
    X_train_img = get_image_features(train_df)
    X_test_img = get_image_features(test_df)

    # 4. Ensemble Training and Prediction
    final_predictions = train_and_predict_ensemble(
        train_df,
        test_df,
        X_train_tfidf,
        X_test_tfidf,
        X_train_img,
        X_test_img
    )

    # 5. Output Generation
    output_df = pd.DataFrame({
        'sample_id': test_df['sample_id'],
        'price': final_predictions
    })

    output_filepath = 'test_out.csv'
    output_df.to_csv(output_filepath, index=False, float_format='%.4f')

    print(f"\n--- SUCCESS: Submission file generated ---")
    print(f"File saved to: {output_filepath}")
    print("Example predictions (first 5 rows):")
    print(output_df.head())

if __name__ == '__main__':
    # Add a check for essential libraries (LightGBM is not always standard in basic environments)
    try:
        run_solution()
    except Exception as e:
        # FileNotFoundError is specifically caught and reported inside load_datasets,
        # but other errors still need generic handling.
        print(f"The script failed to run. Check the error above for details.")
        print("Common issues: Missing CSV files, or missing Python packages (numpy, pandas, lightgbm, scikit-learn).")

from google.colab import drive
drive.mount('/content/drive')



import pandas as pd
import numpy as np
import re
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
# NOTE: In a full environment, you would also import PyTorch/TensorFlow and deep learning utilities

# --- HELPER UTILITY (PLACEHOLDER FOR COMPETITION FILE) ---
# NOTE: The competition requires using a file named src/utils.py for image download.
# You MUST replace this placeholder with the actual implementation provided by the organizers.
def download_images(image_links, output_dir='images'):
    """
    PLACEHOLDER: Simulates the image download function from src/utils.py.
    In the real solution, this function handles downloading the images
    from the provided URLs in 'image_link'.
    """
    import os
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print(f"--- NOTE: Running placeholder for image download. Image features will be mocked. ---")
    print(f"--- You must implement or import the actual 'download_images' from src/utils.py ---")
    # This function would typically return a list of local file paths for the images.
    return [os.path.join(output_dir, f"{i}.jpg") for i in range(len(image_links))]


# --- 1. EVALUATION METRIC ---

def calculate_smape(y_true, y_pred):
    """
    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).
    Formula: (1/n) * Σ(|y_true - y_pred| / ((|y_true| + |y_pred|)/2)) * 100
    """
    # Ensure all predictions are positive (as price must be positive)
    y_pred[y_pred < 0] = 0.0

    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2

    # Handle the case where both true and pred are zero (SMAPE should be 0)
    # This is less likely with real prices, but good practice.
    mask = (y_true == 0) & (y_pred == 0)
    denominator[mask] = 1 # Avoid division by zero, numerator will be 0 anyway.

    smape = np.mean(numerator / denominator) * 100
    return smape

# --- 2. DATA UTILITIES (ACTUAL DATA LOADING) ---

def load_datasets():
    """
    Loads the actual training and testing datasets from the expected file paths.

    The file paths are defined based on the challenge description:
    - Training file: dataset/train.csv
    - Test file: dataset/test.csv
    """
    # NOTE: The paths must be relative to the running environment.
    # Since you provided absolute paths in the error message,
    # I'll revert to the challenge's specified relative path structure:
    TRAIN_PATH = '/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/dataset/train.csv'
    TEST_PATH = '/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/dataset/test.csv'

    print("Loading actual datasets from CSV files...")

    try:
        # Load Training Data (with 'price' label)
        train_df = pd.read_csv(TRAIN_PATH)
        print(f"Loaded training data: {len(train_df)} samples from {TRAIN_PATH}")

        # Load Test Data (without 'price' label)
        test_df = pd.read_csv(TEST_PATH)
        print(f"Loaded test data: {len(test_df)} samples from {TEST_PATH}")

        # Apply the required log transformation for the target variable (price)
        if 'price' in train_df.columns:
            # We use log1p (log(1+x)) to handle potential zero prices robustly
            train_df['log_price'] = np.log1p(train_df['price'])
        else:
            print("WARNING: 'price' column not found in training data. Cannot create log_price target.")

        return train_df, test_df

    except FileNotFoundError as e:
        # NOTE: If you are running with absolute paths (like the ones shown in the log),
        # you need to modify TRAIN_PATH and TEST_PATH above, or symlink the data.
        print(f"ERROR: Could not find dataset file. Please ensure {TRAIN_PATH} and {TEST_PATH} exist in your project structure.")
        raise e

def safe_extract_brand(content):
    """
    Safely extracts the first word after 'Title:' from the catalog content,
    handling potential formatting errors.
    """
    if not isinstance(content, str) or 'Title:' not in content:
        return 'Unknown'

    try:
        # Safely get the part after 'Title:'
        title_part = content.split('Title:', 1)[1].strip()

        # Safely split into words
        words = title_part.split(' ')

        # Take the first non-empty, cleaned word
        for word in words:
            cleaned_word = word.strip().replace(',', '').replace('.', '')
            if cleaned_word:
                return cleaned_word

    except IndexError:
        # Fallback if split fails unexpectedly
        pass

    return 'Unknown'

# --- 3. FEATURE ENGINEERING MODULE ---

def extract_ipq(text):
    """
    Extracts Item Pack Quantity (IPQ) from the catalog content using regex.
    """
    # Look for patterns like 'Pack of 50 bags', '1 monitor', '5 lbs', '30 servings'
    ipq_match = re.search(r'IPQ:\s*(?:Pack of )?(\d+\s*[\w\.]+)', text, re.IGNORECASE)
    if ipq_match:
        return ipq_match.group(1).strip()
    return np.nan

def engineer_features(df, vectorizer=None):
    """
    Engineers tabular and text features.
    """
    print("Starting feature engineering...")

    # Ensure all content is string and handle NaNs before processing
    df['catalog_content'] = df['catalog_content'].fillna('').astype(str)

    # 1. IPQ Extraction (Numerical Feature)
    df['raw_ipq'] = df['catalog_content'].apply(extract_ipq)

    # Extract numerical part of IPQ (very simplified for this example)
    # **Ensure raw_ipq is string before applying .str accessor**
    df['ipq_num'] = df['raw_ipq'].astype(str).str.extract(r'(\d+)').astype(float)
    df['ipq_num'] = df['ipq_num'].fillna(1.0)

    # 2. Brand Extraction (Categorical Feature)
    # **REPLACED COMPLEX LAMBDA WITH ROBUST FUNCTION CALL**
    df['brand'] = df['catalog_content'].apply(safe_extract_brand)

    # Simple Label Encoding for the categorical Brand feature
    df['brand_encoded'] = df['brand'].astype('category').cat.codes

    # 3. TFIDF Vectorization (Textual Feature)
    if vectorizer is None:
        # Fit vectorizer on training data content
        vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))
        tfidf_features = vectorizer.fit_transform(df['catalog_content'])
    else:
        # Transform test data using the fitted vectorizer
        tfidf_features = vectorizer.transform(df['catalog_content'])

    return df, tfidf_features, vectorizer

def get_image_features(df, feature_size=2048):
    """
    MOCK FUNCTION: Simulates feature extraction from images using a pre-trained CNN.
    """
    print(f"--- MOCKING IMAGE FEATURE EXTRACTION ---")

    # In a real environment, you would call:
    # image_paths = download_images(df['image_link'])

    print(f"Using placeholder features of size {feature_size}. In production, this must be a CNN output.")

    # Placeholder: Random features to simulate VGG/ResNet output
    np.random.seed(42)
    features = np.random.rand(len(df), feature_size)
    return features

# --- 4. MULTIMODAL ENSEMBLE MODELING ---

def train_and_predict_ensemble(train_df, test_df, X_train_tfidf, X_test_tfidf, X_train_img, X_test_img):
    """
    Trains the three components (Text GBM, Image NN Placeholder, Ensemble Ridge)
    and generates final predictions.
    """

    # Define features for the GBM (Text Model)
    GBM_FEATURES = ['ipq_num', 'brand_encoded', 'tfidf_sum', 'tfidf_mean']

    # ----------------------------------------------------
    # PHASE 1: TEXT MODEL (LightGBM)
    # ----------------------------------------------------
    print("\n--- Training Text Model (LightGBM) ---")

    # Augment tabular features with simple TFIDF aggregations
    train_df['tfidf_sum'] = X_train_tfidf.sum(axis=1)
    train_df['tfidf_mean'] = X_train_tfidf.mean(axis=1)

    lgb_params = {
        'objective': 'regression_l1', # MAE is a good proxy for SMAPE optimization
        'metric': 'mae',
        'n_estimators': 1000,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 1,
        'verbose': -1,
        'n_jobs': -1,
        'seed': 42
    }

    X_train_gbm = train_df[GBM_FEATURES].values
    y_train = train_df['log_price'].values

    text_model = lgb.LGBMRegressor(**lgb_params)
    text_model.fit(X_train_gbm, y_train)

    # Predict OOF (Out-Of-Fold) for stacking
    text_oof = np.zeros(len(train_df))
    # Note: KFold should be used for robust OOF predictions in production.
    # For simplicity, we use a single prediction here.
    text_oof = text_model.predict(X_train_gbm)

    # Apply TFIDF aggregations to test data for prediction
    test_df['tfidf_sum'] = X_test_tfidf.sum(axis=1)
    test_df['tfidf_mean'] = X_test_tfidf.mean(axis=1)

    text_test_pred = text_model.predict(test_df[GBM_FEATURES].values)

    # ----------------------------------------------------
    # PHASE 2: IMAGE MODEL (Mock Simple FFNN/Linear Model on Image Features)
    # ----------------------------------------------------
    print("\n--- Training Image Model (Linear Regression on CNN Features) ---")

    # Standardize image features for the linear model
    img_scaler = StandardScaler()
    X_train_img_scaled = img_scaler.fit_transform(X_train_img)
    X_test_img_scaled = img_scaler.transform(X_test_img)

    # Use Ridge Regression as a simple, stable model for image features
    image_model = Ridge(alpha=1.0, random_state=42)
    image_model.fit(X_train_img_scaled, y_train)

    image_oof = image_model.predict(X_train_img_scaled)
    image_test_pred = image_model.predict(X_test_img_scaled)

    # ----------------------------------------------------
    # PHASE 3: STACKING / ENSEMBLE MODEL (Meta-Model)
    # ----------------------------------------------------
    print("\n--- Training Ensemble Meta-Model (Stacking) ---")

    # Create the Meta-Features (predictions from base models + key original features)
    X_meta_train = pd.DataFrame({
        'text_pred': text_oof,
        'image_pred': image_oof,
        'ipq_num': train_df['ipq_num'].values
    })

    X_meta_test = pd.DataFrame({
        'text_pred': text_test_pred,
        'image_pred': image_test_pred,
        'ipq_num': test_df['ipq_num'].fillna(train_df['ipq_num'].mean()).values # Handle missing IPQ in test
    })

    # The final ensemble model (Ridge Regression for stable, weighted combination)
    ensemble_model = Ridge(alpha=0.5, random_state=42)
    ensemble_model.fit(X_meta_train, y_train)

    # Final Prediction on the log-scale
    final_log_preds = ensemble_model.predict(X_meta_test)

    # ----------------------------------------------------
    # PHASE 4: INVERSE TRANSFORM AND OUTPUT
    # ----------------------------------------------------

    # Inverse transform (exp(log_price) - 1) to get the final price
    final_price_preds = np.expm1(final_log_preds)

    # Ensure prices are positive floats as per constraint
    final_price_preds[final_price_preds < 0] = 0.01

    return final_price_preds


# --- 5. MAIN EXECUTION ---

def run_solution():
    """Main function to run the entire pricing solution."""

    # 1. Load Data
    train_df, test_df = load_datasets()

    # 2. Feature Engineering (Text/Tabular)
    train_df, X_train_tfidf, tfidf_vectorizer = engineer_features(train_df)
    test_df, X_test_tfidf, _ = engineer_features(test_df, vectorizer=tfidf_vectorizer)

    # 3. Image Feature Extraction (MOCK/Placeholder)
    # NOTE: This is the most computationally expensive and critical part in a real solution
    X_train_img = get_image_features(train_df)
    X_test_img = get_image_features(test_df)

    # 4. Ensemble Training and Prediction
    final_predictions = train_and_predict_ensemble(
        train_df,
        test_df,
        X_train_tfidf,
        X_test_tfidf,
        X_train_img,
        X_test_img
    )

    # 5. Output Generation
    output_df = pd.DataFrame({
        'sample_id': test_df['sample_id'],
        'price': final_predictions
    })

    output_filepath = 'test_out.csv'
    output_df.to_csv(output_filepath, index=False, float_format='%.4f')

    print(f"\n--- SUCCESS: Submission file generated ---")
    print(f"File saved to: {output_filepath}")
    print("Example predictions (first 5 rows):")
    print(output_df.head())

if __name__ == '__main__':
    # Add a check for essential libraries (LightGBM is not always standard in basic environments)
    try:
        run_solution()
    except Exception as e:
        # FileNotFoundError is specifically caught and reported inside load_datasets,
        # but other errors still need generic handling.
        print(f"The script failed to run. Check the error above for details.")
        print(f"Specific Error Trace: {e}")
        print("Common issues: Missing CSV files, or missing Python packages (numpy, pandas, lightgbm, scikit-learn).")

import pandas as pd

try:
    output_df = pd.read_csv('test_out.csv')
    print("Contents of test_out.csv:")
    display(output_df.head())
    print(f"\nFile shape: {output_df.shape}")
except FileNotFoundError:
    print("Error: test_out.csv not found. Please ensure the main script ran successfully to generate this file.")
except Exception as e:
    print(f"An error occurred while reading test_out.csv: {e}")