# -*- coding: utf-8 -*-
"""GRU-Amazon-ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PnHSST2lXQ1HGo2uayod80G7i39flYY8
"""

from google.colab import drive
drive.mount('/content/drive')
# Imports
import os
import re
import string
import numpy as np
import pandas as pd
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout
import tensorflow as tf

# Load data
train = pd.read_csv('/content/drive/MyDrive/Amazon_ML/student_resource/dataset/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Amazon_ML/student_resource/dataset/test.csv')

# Clean text
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower().strip()
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)
    emoji_pattern = re.compile("[" u"\U0001F600-\U0001F64F" u"\U0001F300-\U0001F5FF"
                               u"\U0001F680-\U0001F6FF" u"\U0001F1E0-\U0001F1FF"
                               u"\U00002700-\U000027BF" u"\U0001F900-\U0001F9FF" "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)
    text = re.sub(r"[^a-z0-9\s\+\-x%./]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

train['clean_text'] = train['catalog_content'].fillna("").apply(clean_text)
test['clean_text'] = test['catalog_content'].fillna("").apply(clean_text)

# Tokenization
MAX_NUM_WORDS = 20000
MAX_SEQ_LEN = 100

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(train['clean_text'])

X_train_seq = tokenizer.texts_to_sequences(train['clean_text'])
X_test_seq = tokenizer.texts_to_sequences(test['clean_text'])

X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

y_train = train['price'].values
X_tr, X_val, y_tr, y_val = train_test_split(X_train_pad, y_train, test_size=0.2, random_state=42)

# Build GRU Model
EMBEDDING_DIM = 100

model = Sequential([
    Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQ_LEN),
    GRU(128, return_sequences=True),
    Dropout(0.3),
    GRU(64, return_sequences=False),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_absolute_error')
model.summary()

# Train
history = model.fit(
    X_tr, y_tr,
    validation_data=(X_val, y_val),
    epochs=7,
    batch_size=128
)

# Evaluate
y_val_pred = model.predict(X_val).flatten()
mae = mean_absolute_error(y_val, y_val_pred)
print(f"Validation MAE: {mae:.2f}")

# Predict on test set
test_preds = model.predict(X_test_pad).flatten()
test_preds = np.maximum(test_preds, 0)

# Save model and tokenizer
model.save('/content/drive/MyDrive/Amazon_ML/gru_model.h5')

import pickle
with open('/content/drive/MyDrive/Amazon_ML/tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

import os
import pandas as pd
import numpy as np
import pickle
import re
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

# ===========================
# ðŸ”§ CONFIG
# ===========================
DATASET_FOLDER = '/content/drive/MyDrive/Amazon_ML/student_resource/dataset/'
MODEL_PATH = '/content/drive/MyDrive/Amazon_ML/gru_model.h5'
TOKENIZER_PATH = '/content/drive/MyDrive/Amazon_ML/tokenizer.pkl'
MAX_SEQ_LEN = 100  # Make sure this matches your training setting

# ===========================
# ðŸ§¹ Clean Function
# ===========================
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower().strip()
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)
    text = re.sub(r"[^a-z0-9\s\+\-x%./]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ===========================
# ðŸš€ Fast Batch Prediction
# ===========================
def run_batch_prediction():
    print("ðŸ”„ Loading model and tokenizer...")
    model = load_model(MODEL_PATH)

    with open(TOKENIZER_PATH, 'rb') as f:
        tokenizer = pickle.load(f)

    print("ðŸ“„ Reading test.csv...")
    test_df = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))

    print("ðŸ§¹ Cleaning text data...")
    test_df['cleaned_text'] = test_df['catalog_content'].fillna("").apply(clean_text)

    print("ðŸ”¢ Tokenizing...")
    sequences = tokenizer.texts_to_sequences(test_df['cleaned_text'])
    padded = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

    print("ðŸ§  Predicting...")
    preds = model.predict(padded).flatten()
    test_df['price'] = np.maximum(preds, 0).round(2)

    output_df = test_df[['sample_id', 'price']]
    output_path = os.path.join(DATASET_FOLDER, 'test_out.csv')
    output_df.to_csv(output_path, index=False)

    print(f"âœ… Predictions saved to {output_path}")
    print("ðŸ“Š Sample predictions:\n", output_df.head())

# ===========================
# âœ… Main
# ===========================
if __name__ == "__main__":
    run_batch_prediction()

from sklearn.metrics import mean_absolute_error

def smape(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_pred) + np.abs(y_true)) / 2
    return np.mean(numerator / denominator) * 100

# Predict on validation set
y_val_pred = model.predict(X_val).flatten()

# Calculate metrics
mae = mean_absolute_error(y_val, y_val_pred)
smape_val = smape(y_val, y_val_pred)

print(f"Validation MAE: {mae:.2f}")
print(f"Validation SMAPE: {smape_val:.2f}%")

